{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installing and Importing Required Libraries"
      ],
      "metadata": {
        "id": "Hz9vzz1oZ6fg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2fKNUP3YHgV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# To Display plots\n",
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load and Inspect the CSV Data"
      ],
      "metadata": {
        "id": "g-1BYi7taAbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv(\"timeline_data.csv\")\n",
        "\n",
        "print(\"Raw Data (first 5 rows):\")\n",
        "display(df_raw.head())\n",
        "\n",
        "print(\"\\nColumns Detected:\", df_raw.columns.tolist())\n",
        "\n",
        "# Make a working copy\n",
        "df = df_raw.copy()"
      ],
      "metadata": {
        "id": "Gx7N4ZvuYabk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Cleaning"
      ],
      "metadata": {
        "id": "irKv86NfaKhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Clean & Parse 'Month' Column"
      ],
      "metadata": {
        "id": "khBRBs4LaMTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_raw.copy()  # Work on a copy\n",
        "# Remove leading/trailing spaces\n",
        "df[\"Month\"] = df[\"Month\"].astype(str).str.strip()\n",
        "# Remove internal spaces if any\n",
        "df[\"Month\"] = df[\"Month\"].str.replace(r\"\\s+\", \"\", regex=True)\n",
        "# Convert \"Jan-04\" style strings to datetime\n",
        "# format='%b-%y' means \"abbreviated month - 2-digit year\"\n",
        "df[\"Month\"] = pd.to_datetime(df[\"Month\"], format=\"%b-%y\", errors=\"coerce\")\n",
        "\n",
        "# Check if any rows did not parse properly\n",
        "bad_rows = df[df[\"Month\"].isna()]\n",
        "if not bad_rows.empty:\n",
        "    print(\"⚠️ Some rows could not be parsed as dates:\")\n",
        "    display(bad_rows)\n",
        "else:\n",
        "    print(\"✅ All Month values parsed successfully.\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rdMqE3hYYctH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Replace \"<1\" and Convert Keyword Columns to Float"
      ],
      "metadata": {
        "id": "rythu0ojaO-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = [\n",
        "    \"Smartphone: (Worldwide)\",\n",
        "    \"Artificial intelligence: (Worldwide)\",\n",
        "    \"Wearable technology: (Worldwide)\",\n",
        "    \"Online shopping: (Worldwide)\",\n",
        "    \"Electric vehicle: (Worldwide)\"\n",
        "]\n",
        "\n",
        "for col in keywords:\n",
        "    df[col] = df[col].replace(\"<1\", 0.5).astype(float)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "r1uJlZILYfOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Create a Tech_Wave Label (Tech 2.0 if before 2016, Tech 3.0 otherwise)"
      ],
      "metadata": {
        "id": "z8RTToQlaVNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Tech_Wave\"] = df[\"Month\"].apply(\n",
        "    lambda x: \"Tech 2.0\" if x < pd.Timestamp(\"2016-01-01\") else \"Tech 3.0\"\n",
        ")\n",
        "\n",
        "print(\"\\nCleaned Data (first 5 rows):\")\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "WPjsYzvcYhOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "3IaVY0ONaabL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "for col in keywords:\n",
        "    plt.plot(df[\"Month\"], df[col], label=col.split(\":\")[0])\n",
        "plt.axvline(pd.Timestamp(\"2016-01-01\"), color=\"red\", linestyle=\"--\", label=\"Start Tech 3.0\")\n",
        "plt.title(\"Consumer Interest Over Time (Google Trends)\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Search Interest (0-100 scale)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RnDySWLyYhNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Clustering and Classification for Tech 2.0 and Tech 3.0"
      ],
      "metadata": {
        "id": "_9u8x_2iaeEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_clustering_classification(data, wave_label):\n",
        "    print(f\"Analyzing {wave_label} Data\")\n",
        "\n",
        "    # Extract features\n",
        "    X = data[keywords].values\n",
        "\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Clustering\n",
        "    # Choose number of clusters (e.g., 3 clusters)\n",
        "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "    data[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    # Display cluster averages to understand characteristics\n",
        "    cluster_means = data.groupby(\"Cluster\")[keywords].mean()\n",
        "    print(\"Cluster Averages:\")\n",
        "    display(cluster_means)\n",
        "\n",
        "    # Plot bar charts for each cluster's keyword averages\n",
        "    cluster_means.plot(kind=\"bar\", figsize=(10, 6))\n",
        "    plt.title(f\"Average Consumer Interest per Cluster in {wave_label}\")\n",
        "    plt.xlabel(\"Cluster\")\n",
        "    plt.ylabel(\"Average Interest\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Classification\n",
        "    # Here we treat the cluster labels as the target variable.\n",
        "    # Our goal: How well can a classifier predict a month’s cluster based on its features?\n",
        "    X_class = X_scaled\n",
        "    y_class = data[\"Cluster\"]\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_class, y_class, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    clf = RandomForestClassifier(random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(\"\\nClassification Report (Predicting Cluster)\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Feature Importance Plot\n",
        "    importances = pd.Series(clf.feature_importances_, index=keywords)\n",
        "    importances.sort_values().plot(kind=\"barh\", figsize=(8, 4), color=\"skyblue\")\n",
        "    plt.title(f\"Feature Importance for Predicting Cluster in {wave_label}\")\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.ylabel(\"Keyword\")\n",
        "    plt.show()\n",
        "\n",
        "# Subset data for each tech wave\n",
        "df_tech2 = df[df[\"Tech_Wave\"] == \"Tech 2.0\"].copy()\n",
        "df_tech3 = df[df[\"Tech_Wave\"] == \"Tech 3.0\"].copy()\n",
        "\n",
        "# Run clustering and classification for Tech 2.0\n",
        "run_clustering_classification(df_tech2, \"Tech 2.0\")\n",
        "\n",
        "# Run clustering and classification for Tech 3.0\n",
        "run_clustering_classification(df_tech3, \"Tech 3.0\")\n"
      ],
      "metadata": {
        "id": "lAmMHcPeYhLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hdh9aioqYhJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZRjyqmRYhHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "deawUJ1nYhDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}